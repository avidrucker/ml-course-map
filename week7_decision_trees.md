# Week 7: Decision Trees
**Decision Trees (topic)**  
- (assertion) To create a decision tree using scikit-learn, one must initialize a DecisionTreeClassifier, fit it to the training data, and optionally specify a criterion such as “gini” for splitting.  
- (assertion) “Gini” index measures node impurity, guiding the decision tree in choosing splits that best reduce impurity.  
- (assertion) A pure node is one where all samples belong to the same class, indicating no further splitting is needed.  
- (task) Implement a decision tree classifier in a Jupyter Notebook:  
  - construct the model,  
  - fit it on the training set,  
  - predict on test data,  
  - evaluate performance (accuracy, precision, recall, F1).

- (assertion) Decision trees are particularly effective when data have strong, rule-like structures and can be easily interpreted. They may not be ideal for very high-dimensional data or when smooth decision boundaries are needed.
